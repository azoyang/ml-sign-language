{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bithandsonmlconda3cac151bf45540d1b166a9381c42e0c9",
   "display_name": "Python 3.7.7 64-bit ('hands-on-ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          x0         y0       z0         x1         y1       z1         x2  \\\n0  284.92805  350.67788 -0.00093  322.79277  315.33405  8.39052  347.40954   \n1  278.28085  345.40205 -0.00130  318.74006  314.35025  6.34735  346.35429   \n2  276.40025  344.67422 -0.00131  317.00817  313.70587  5.75395  344.34662   \n3  274.34967  343.92245 -0.00131  317.74627  312.19434  7.38592  345.21930   \n4  273.91013  343.22628 -0.00133  320.29775  310.32568  7.56058  349.07755   \n\n          y2        z2         x3  ...        x18        y18      z18  \\\n0  277.24308  11.12329  372.08923  ...  286.79234  172.75314 -6.02986   \n1  277.35317   8.47528  371.35733  ...  291.32571  172.43149 -7.07840   \n2  276.55692   7.94423  366.93927  ...  289.30880  173.07098 -7.00137   \n3  274.20845  10.34781  368.13793  ...  290.49569  170.87830 -5.92087   \n4  272.35242  10.71873  373.16762  ...  291.84546  169.00737 -5.37861   \n\n         x19        y19      z19        x20        y20      z20  clase  \n0  295.90823  144.13841 -4.17843  305.38755  114.55426 -2.41008      0  \n1  299.57190  145.29706 -6.46456  307.54150  116.75922 -6.23263      0  \n2  296.22140  146.44763 -6.53205  303.59922  118.51561 -6.35039      0  \n3  299.04903  143.51583 -5.19691  307.87086  115.32076 -4.96793      0  \n4  299.95371  141.54401 -4.62802  307.74208  113.48242 -4.36697      0  \n\n[5 rows x 64 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>y0</th>\n      <th>z0</th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>z1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>z2</th>\n      <th>x3</th>\n      <th>...</th>\n      <th>x18</th>\n      <th>y18</th>\n      <th>z18</th>\n      <th>x19</th>\n      <th>y19</th>\n      <th>z19</th>\n      <th>x20</th>\n      <th>y20</th>\n      <th>z20</th>\n      <th>clase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>284.92805</td>\n      <td>350.67788</td>\n      <td>-0.00093</td>\n      <td>322.79277</td>\n      <td>315.33405</td>\n      <td>8.39052</td>\n      <td>347.40954</td>\n      <td>277.24308</td>\n      <td>11.12329</td>\n      <td>372.08923</td>\n      <td>...</td>\n      <td>286.79234</td>\n      <td>172.75314</td>\n      <td>-6.02986</td>\n      <td>295.90823</td>\n      <td>144.13841</td>\n      <td>-4.17843</td>\n      <td>305.38755</td>\n      <td>114.55426</td>\n      <td>-2.41008</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>278.28085</td>\n      <td>345.40205</td>\n      <td>-0.00130</td>\n      <td>318.74006</td>\n      <td>314.35025</td>\n      <td>6.34735</td>\n      <td>346.35429</td>\n      <td>277.35317</td>\n      <td>8.47528</td>\n      <td>371.35733</td>\n      <td>...</td>\n      <td>291.32571</td>\n      <td>172.43149</td>\n      <td>-7.07840</td>\n      <td>299.57190</td>\n      <td>145.29706</td>\n      <td>-6.46456</td>\n      <td>307.54150</td>\n      <td>116.75922</td>\n      <td>-6.23263</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>276.40025</td>\n      <td>344.67422</td>\n      <td>-0.00131</td>\n      <td>317.00817</td>\n      <td>313.70587</td>\n      <td>5.75395</td>\n      <td>344.34662</td>\n      <td>276.55692</td>\n      <td>7.94423</td>\n      <td>366.93927</td>\n      <td>...</td>\n      <td>289.30880</td>\n      <td>173.07098</td>\n      <td>-7.00137</td>\n      <td>296.22140</td>\n      <td>146.44763</td>\n      <td>-6.53205</td>\n      <td>303.59922</td>\n      <td>118.51561</td>\n      <td>-6.35039</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>274.34967</td>\n      <td>343.92245</td>\n      <td>-0.00131</td>\n      <td>317.74627</td>\n      <td>312.19434</td>\n      <td>7.38592</td>\n      <td>345.21930</td>\n      <td>274.20845</td>\n      <td>10.34781</td>\n      <td>368.13793</td>\n      <td>...</td>\n      <td>290.49569</td>\n      <td>170.87830</td>\n      <td>-5.92087</td>\n      <td>299.04903</td>\n      <td>143.51583</td>\n      <td>-5.19691</td>\n      <td>307.87086</td>\n      <td>115.32076</td>\n      <td>-4.96793</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>273.91013</td>\n      <td>343.22628</td>\n      <td>-0.00133</td>\n      <td>320.29775</td>\n      <td>310.32568</td>\n      <td>7.56058</td>\n      <td>349.07755</td>\n      <td>272.35242</td>\n      <td>10.71873</td>\n      <td>373.16762</td>\n      <td>...</td>\n      <td>291.84546</td>\n      <td>169.00737</td>\n      <td>-5.37861</td>\n      <td>299.95371</td>\n      <td>141.54401</td>\n      <td>-4.62802</td>\n      <td>307.74208</td>\n      <td>113.48242</td>\n      <td>-4.36697</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 64 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_csv('./data/datos.csv', encoding = \"UTF-8\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop(['clase'], axis=1), data['clase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10203, 63)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_preprocessor = make_pipeline(\n",
    "    MinMaxScaler(feature_range=(0,1))\n",
    ")\n",
    "\n",
    "z_preprocessor = make_pipeline(\n",
    "    MinMaxScaler(feature_range=(0,1))\n",
    ")\n",
    "\n",
    "x_columns = ('x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20')\n",
    "y_columns = ('y0','y1','y2','y3','y4','y5','y6','y7','y8','y9','y10','y11','y12','y13','y14','y15','y16','y17','y18','y19','y20')\n",
    "z_columns = ('z0','z1','z2','z3','z4','z5','z6','z7','z8','z9','z10','z11','z12','z13','z14','z15','z16','z17','z18','z19','z20')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (xy_preprocessor, x_columns),\n",
    "    (xy_preprocessor, y_columns),\n",
    "    (z_preprocessor, z_columns),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(n_jobs=-1, early_stopping=True, verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "e after 6 epochs took 0.13 seconds\n\nConvergence after 6 epochs took 0.12 seconds\n\nNorm: 0.06, NNZs: 63, Bias: -0.765151, T: 44076, Avg. loss: 0.149836\nTotal training time: 0.13 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.771443, T: 44076, Avg. loss: 0.172984Convergence after 6 epochs took 0.13 seconds\nTotal training time: 0.13 seconds.\n\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.13 seconds\nConvergence after 6 epochs took 0.13 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.14 seconds\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.1s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done   3 out of  11 | elapsed:    0.1s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    0.1s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done   5 out of  11 | elapsed:    0.1s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done   6 out of  11 | elapsed:    0.1s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   7 out of  11 | elapsed:    0.1s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.1s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   9 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s finished\nUsing ThreadingBackend as joblib.Parallel backend instead of LokyBackend as the latter does not provide shared memory semantics.\n-- Epoch 1\n-- Epoch 1Norm: 0.07, NNZs: 63, Bias: -0.725029, T: 7346, Avg. loss: 0.192975-- Epoch 1-- Epoch 1\n\n\n\nTotal training time: 0.00 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.733527, T: 7346, Avg. loss: 0.193042\nTotal training time: 0.00 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.732964, T: 7346, Avg. loss: 0.158667\nTotal training time: 0.01 seconds.\n-- Epoch 1Norm: 0.07, NNZs: 63, Bias: -0.746509, T: 7346, Avg. loss: 0.193003-- Epoch 1\n\n\nTotal training time: 0.01 seconds.\n-- Epoch 1-- Epoch 1-- Epoch 1\nNorm: 0.07, NNZs: 63, Bias: -0.739499, T: 7346, Avg. loss: 0.191087-- Epoch 1\n\n\n\nNorm: 0.07, NNZs: 63, Bias: -0.711512, T: 7346, Avg. loss: 0.176636\nTotal training time: 0.01 seconds.\nTotal training time: 0.01 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.710464, T: 7346, Avg. loss: 0.153506\nNorm: 0.07, NNZs: 63, Bias: -0.728794, T: 7346, Avg. loss: 0.157035Norm: 0.07, NNZs: 63, Bias: -0.732481, T: 7346, Avg. loss: 0.166120\nTotal training time: 0.00 seconds.\nTotal training time: 0.00 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.726483, T: 7346, Avg. loss: 0.170882\nTotal training time: 0.01 seconds.\n\n-- Epoch 1\nTotal training time: 0.01 seconds.Norm: 0.08, NNZs: 63, Bias: -0.706939, T: 7346, Avg. loss: 0.151044\n\nTotal training time: 0.00 seconds.\n-- Epoch 2\n-- Epoch 2-- Epoch 2\n\n-- Epoch 2\nNorm: 0.06, NNZs: 63, Bias: -0.759767, T: 14692, Avg. loss: 0.191513-- Epoch 2Norm: 0.07, NNZs: 63, Bias: -0.739456, T: 14692, Avg. loss: 0.189422\nTotal training time: 0.03 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.741384, T: 14692, Avg. loss: 0.156016\nTotal training time: 0.03 seconds.\n-- Epoch 2\nTotal training time: 0.02 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.746313, T: 14692, Avg. loss: 0.157377\n\nTotal training time: 0.03 seconds.\n\n-- Epoch 2\n-- Epoch 2-- Epoch 2-- Epoch 2\n\n\nNorm: 0.07, NNZs: 63, Bias: -0.721616, T: 14692, Avg. loss: 0.147890\nTotal training time: 0.02 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.752616, T: 14692, Avg. loss: 0.189709Norm: 0.07, NNZs: 63, Bias: -0.739719, T: 14692, Avg. loss: 0.169615\nTotal training time: 0.03 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.747199, T: 14692, Avg. loss: 0.191263-- Epoch 2\nTotal training time: 0.04 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.724042, T: 14692, Avg. loss: 0.151261Norm: 0.07, NNZs: 63, Bias: -0.726004, T: 14692, Avg. loss: 0.174870\nTotal training time: 0.03 seconds.\n\n\nTotal training time: 0.03 seconds.\n\nTotal training time: 0.03 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.745211, T: 14692, Avg. loss: 0.165028\nTotal training time: 0.03 seconds.\n-- Epoch 3\nNorm: 0.06, NNZs: 63, Bias: -0.767146, T: 22038, Avg. loss: 0.191010\nTotal training time: 0.05 seconds.\n-- Epoch 3\n-- Epoch 3-- Epoch 3\n\nNorm: 0.06, NNZs: 63, Bias: -0.753724, T: 22038, Avg. loss: 0.156973-- Epoch 3Norm: 0.06, NNZs: 63, Bias: -0.748545, T: 22038, Avg. loss: 0.155771\nTotal training time: 0.04 seconds.\n\n\nNorm: 0.06, NNZs: 63, Bias: -0.760126, T: 22038, Avg. loss: 0.189373\nTotal training time: 0.05 seconds.\nTotal training time: 0.05 seconds.\n-- Epoch 3\nNorm: 0.07, NNZs: 63, Bias: -0.730024, T: 22038, Avg. loss: 0.147427-- Epoch 3Norm: 0.06, NNZs: 63, Bias: -0.747368, T: 22038, Avg. loss: 0.188811\nTotal training time: 0.06 seconds.\n-- Epoch 3\n-- Epoch 3\n\n\n-- Epoch 3-- Epoch 3Total training time: 0.04 seconds.\n\n\nNorm: 0.06, NNZs: 63, Bias: -0.747100, T: 22038, Avg. loss: 0.169228Norm: 0.06, NNZs: 63, Bias: -0.754973, T: 22038, Avg. loss: 0.190827\nTotal training time: 0.05 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.734064, T: 22038, Avg. loss: 0.174390\nTotal training time: 0.05 seconds.\n\nNorm: 0.07, NNZs: 63, Bias: -0.731648, T: 22038, Avg. loss: 0.150901Norm: 0.06, NNZs: 63, Bias: -0.752163, T: 22038, Avg. loss: 0.164672Total training time: 0.06 seconds.\n\nTotal training time: 0.05 seconds.\n\nTotal training time: 0.05 seconds.\n-- Epoch 4-- Epoch 4\n\nNorm: 0.06, NNZs: 63, Bias: -0.753474, T: 29384, Avg. loss: 0.155565\nTotal training time: 0.06 seconds.\n-- Epoch 4Norm: 0.06, NNZs: 63, Bias: -0.772494, T: 29384, Avg. loss: 0.190812\n-- Epoch 4-- Epoch 4\nTotal training time: 0.07 seconds.\n-- Epoch 4\n\n\nNorm: 0.07, NNZs: 63, Bias: -0.735768, T: 29384, Avg. loss: 0.147115\nTotal training time: 0.06 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.758998, T: 29384, Avg. loss: 0.156794Norm: 0.06, NNZs: 63, Bias: -0.765291, T: 29384, Avg. loss: 0.189089\nTotal training time: 0.07 seconds.\n\nTotal training time: 0.07 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.753014, T: 29384, Avg. loss: 0.188515\nTotal training time: 0.08 seconds.\n-- Epoch 4\n-- Epoch 4\n-- Epoch 4-- Epoch 4\n-- Epoch 4Norm: 0.07, NNZs: 63, Bias: -0.739727, T: 29384, Avg. loss: 0.174137\nTotal training time: 0.07 seconds.\n\n\nNorm: 0.06, NNZs: 63, Bias: -0.757260, T: 29384, Avg. loss: 0.164620\nTotal training time: 0.07 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.760278, T: 29384, Avg. loss: 0.190526Norm: 0.06, NNZs: 63, Bias: -0.752380, T: 29384, Avg. loss: 0.169104\nTotal training time: 0.07 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.737049, T: 29384, Avg. loss: 0.150786\nTotal training time: 0.07 seconds.\n\nTotal training time: 0.08 seconds.\n-- Epoch 5-- Epoch 5\n-- Epoch 5-- Epoch 5\n\n-- Epoch 5\nNorm: 0.06, NNZs: 63, Bias: -0.757353, T: 36730, Avg. loss: 0.155499Norm: 0.06, NNZs: 63, Bias: -0.776451, T: 36730, Avg. loss: 0.190517\n\nTotal training time: 0.08 seconds.\n\nTotal training time: 0.09 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.740223, T: 36730, Avg. loss: 0.146917\nTotal training time: 0.07 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.762980, T: 36730, Avg. loss: 0.156605\nTotal training time: 0.09 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.769249, T: 36730, Avg. loss: 0.188959\nTotal training time: 0.09 seconds.\n-- Epoch 5\n-- Epoch 5\nNorm: 0.07, NNZs: 63, Bias: -0.743967, T: 36730, Avg. loss: 0.173888\nTotal training time: 0.09 seconds.\n-- Epoch 5\nNorm: 0.06, NNZs: 63, Bias: -0.756277, T: 36730, Avg. loss: 0.168875-- Epoch 5\nNorm: 0.06, NNZs: 63, Bias: -0.757447, T: 36730, Avg. loss: 0.188351-- Epoch 5\nTotal training time: 0.10 seconds.\n\n\nTotal training time: 0.09 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.761032, T: 36730, Avg. loss: 0.164447\nTotal training time: 0.09 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.764402, T: 36730, Avg. loss: 0.190353\nTotal training time: 0.10 seconds.\n-- Epoch 6\n-- Epoch 6-- Epoch 6\n\n-- Epoch 5\nNorm: 0.06, NNZs: 63, Bias: -0.760433, T: 44076, Avg. loss: 0.155366\nTotal training time: 0.10 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.772427, T: 44076, Avg. loss: 0.188810-- Epoch 6\n\nTotal training time: 0.10 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.766197, T: 44076, Avg. loss: 0.156460Norm: 0.07, NNZs: 63, Bias: -0.741025, T: 36730, Avg. loss: 0.150541\nTotal training time: 0.10 seconds.\n-- Epoch 6\nTotal training time: 0.11 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.779706, T: 44076, Avg. loss: 0.190382-- Epoch 6\n\n\nTotal training time: 0.11 seconds.\nNorm: 0.07, NNZs: 63, Bias: -0.747450, T: 44076, Avg. loss: 0.173778Norm: 0.07, NNZs: 63, Bias: -0.743791, T: 44076, Avg. loss: 0.146736\nTotal training time: 0.11 seconds.\n\nTotal training time: 0.10 seconds.\n-- Epoch 6\n-- Epoch 6\nNorm: 0.06, NNZs: 63, Bias: -0.759526, T: 44076, Avg. loss: 0.168812-- Epoch 6\n\nTotal training time: 0.11 seconds.\nConvergence after 6 epochs took 0.11 seconds\nNorm: 0.06, NNZs: 63, Bias: -0.767693, T: 44076, Avg. loss: 0.190170-- Epoch 6\nTotal training time: 0.12 seconds.\nNorm: 0.06, NNZs: 63, Bias: -0.764124, T: 44076, Avg. loss: 0.164367\n\nTotal training time: 0.12 seconds.\n-- Epoch 6\nConvergence after 6 epochs took 0.13 seconds\nNorm: 0.07, NNZs: 63, Bias: -0.744334, T: 44076, Avg. loss: 0.150514Norm: 0.06, NNZs: 63, Bias: -0.760919, T: 44076, Avg. loss: 0.188113\nTotal training time: 0.12 seconds.\n\nTotal training time: 0.13 seconds.\nConvergence after 6 epochs took 0.12 secondsConvergence after 6 epochs took 0.12 secondsConvergence after 6 epochs took 0.13 seconds\n\n\nConvergence after 6 epochs took 0.12 seconds\nConvergence after 6 epochs took 0.13 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.15 seconds\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.1s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done   3 out of  11 | elapsed:    0.1s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    0.1s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done   5 out of  11 | elapsed:    0.1s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done   6 out of  11 | elapsed:    0.1s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   7 out of  11 | elapsed:    0.1s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.2s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   9 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.2s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done   3 out of  11 | elapsed:    0.2s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    0.2s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done   5 out of  11 | elapsed:    0.2s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done   6 out of  11 | elapsed:    0.2s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   7 out of  11 | elapsed:    0.2s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.2s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done   9 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s finished\n-- Epoch 1\n-- Epoch 1-- Epoch 1\n\nNorm: 56.45, NNZs: 63, Bias: -23.602421, T: 9182, Avg. loss: 0.687156-- Epoch 1-- Epoch 1-- Epoch 1\n-- Epoch 1\n\n\nTotal training time: 0.01 seconds.\nNorm: 84.87, NNZs: 63, Bias: -58.319654, T: 9182, Avg. loss: 1.530609\n-- Epoch 1-- Epoch 1\n\n\nNorm: 76.76, NNZs: 63, Bias: 3.125477, T: 9182, Avg. loss: 2.840103Norm: 68.25, NNZs: 63, Bias: -15.473772, T: 9182, Avg. loss: 1.588268Norm: 59.85, NNZs: 63, Bias: 23.610085, T: 9182, Avg. loss: 2.142229Total training time: 0.01 seconds.\nNorm: 75.96, NNZs: 63, Bias: -20.209052, T: 9182, Avg. loss: 2.385587\nTotal training time: 0.00 seconds.\nNorm: 60.16, NNZs: 63, Bias: 9.359377, T: 9182, Avg. loss: 2.475854\n\nTotal training time: 0.00 seconds.\n-- Epoch 1Norm: 80.27, NNZs: 63, Bias: -33.373541, T: 9182, Avg. loss: 2.483726\nNorm: 82.72, NNZs: 63, Bias: -67.394858, T: 9182, Avg. loss: 1.907114\n-- Epoch 1\nTotal training time: 0.01 seconds.\nTotal training time: 0.01 seconds.\n\n\nTotal training time: 0.01 seconds.Total training time: 0.00 seconds.\n\n\nTotal training time: 0.01 seconds.\nNorm: 69.64, NNZs: 63, Bias: -0.689606, T: 9182, Avg. loss: 2.658716\nTotal training time: 0.00 seconds.\nNorm: 40.37, NNZs: 63, Bias: -31.996114, T: 9182, Avg. loss: 0.227141\nTotal training time: 0.00 seconds.\n-- Epoch 2\n-- Epoch 2-- Epoch 2\n\n-- Epoch 2\n-- Epoch 2-- Epoch 2\n\nNorm: 46.22, NNZs: 63, Bias: 2.778136, T: 18364, Avg. loss: 0.654300Norm: 38.22, NNZs: 63, Bias: -20.974697, T: 18364, Avg. loss: 0.048167Norm: 46.79, NNZs: 63, Bias: 16.047034, T: 18364, Avg. loss: 0.501686\nTotal training time: 0.03 seconds.\n\nTotal training time: 0.03 seconds.\n\nTotal training time: 0.03 seconds.\nNorm: 57.53, NNZs: 63, Bias: 0.140510, T: 18364, Avg. loss: 0.677130\nTotal training time: 0.02 seconds.\nNorm: 61.84, NNZs: 63, Bias: -48.140688, T: 18364, Avg. loss: 0.208253-- Epoch 2-- Epoch 2\nTotal training time: 0.03 seconds.\n\n\n-- Epoch 2-- Epoch 2Norm: 62.45, NNZs: 63, Bias: -26.541519, T: 18364, Avg. loss: 0.370955\n\nTotal training time: 0.03 seconds.\nNorm: 56.81, NNZs: 63, Bias: -18.618961, T: 18364, Avg. loss: 0.493084\n-- Epoch 2\nTotal training time: 0.03 seconds.\n\nNorm: 23.62, NNZs: 63, Bias: -31.400251, T: 18364, Avg. loss: 0.003440Norm: 61.88, NNZs: 63, Bias: -59.913819, T: 18364, Avg. loss: 0.197230\nTotal training time: 0.03 seconds.\n\nTotal training time: 0.02 seconds.\nNorm: 50.55, NNZs: 63, Bias: 0.070899, T: 18364, Avg. loss: 0.568150\nTotal training time: 0.03 seconds.\nNorm: 51.10, NNZs: 63, Bias: -15.665794, T: 18364, Avg. loss: 0.318508\nTotal training time: 0.03 seconds.\n-- Epoch 3\n-- Epoch 3-- Epoch 3\n\n-- Epoch 3-- Epoch 3\nNorm: 51.41, NNZs: 63, Bias: -41.286477, T: 27546, Avg. loss: 0.135250Norm: 48.33, NNZs: 63, Bias: -2.014021, T: 27546, Avg. loss: 0.449901Norm: 38.89, NNZs: 63, Bias: 3.244531, T: 27546, Avg. loss: 0.441274\nTotal training time: 0.05 seconds.\n\nTotal training time: 0.05 seconds.\n\n\nTotal training time: 0.05 seconds.\n-- Epoch 3Norm: 54.29, NNZs: 63, Bias: -21.482938, T: 27546, Avg. loss: 0.201291\n\nTotal training time: 0.05 seconds.\n-- Epoch 3\n-- Epoch 3Norm: 30.51, NNZs: 63, Bias: -19.365675, T: 27546, Avg. loss: 0.027502\n-- Epoch 3Norm: 48.81, NNZs: 63, Bias: -16.187264, T: 27546, Avg. loss: 0.300879\nTotal training time: 0.05 seconds.\n\n\nNorm: 40.84, NNZs: 63, Bias: 13.097838, T: 27546, Avg. loss: 0.315380Total training time: 0.06 seconds.\nTotal training time: 0.05 seconds.\n\nNorm: 54.17, NNZs: 63, Bias: -52.298701, T: 27546, Avg. loss: 0.108741\nTotal training time: 0.05 seconds.\nNorm: 41.63, NNZs: 63, Bias: -15.183295, T: 27546, Avg. loss: 0.189471\nTotal training time: 0.05 seconds.\n-- Epoch 3\n-- Epoch 3\nNorm: 44.65, NNZs: 63, Bias: 0.037595, T: 27546, Avg. loss: 0.351487\nTotal training time: 0.05 seconds.\nNorm: 18.98, NNZs: 63, Bias: -30.091896, T: 27546, Avg. loss: 0.003133\nTotal training time: 0.05 seconds.\n-- Epoch 4-- Epoch 4\n\nNorm: 48.30, NNZs: 63, Bias: -19.127753, T: 36728, Avg. loss: 0.130937Norm: 45.67, NNZs: 63, Bias: -36.004899, T: 36728, Avg. loss: 0.098055\nTotal training time: 0.06 seconds.\n-- Epoch 4\nTotal training time: 0.07 seconds.\n\n-- Epoch 4\n-- Epoch 4-- Epoch 4\n-- Epoch 4\nNorm: 35.44, NNZs: 63, Bias: 3.188041, T: 36728, Avg. loss: 0.341182Norm: 43.51, NNZs: 63, Bias: -2.506548, T: 36728, Avg. loss: 0.324531-- Epoch 4\nTotal training time: 0.07 seconds.\nNorm: 37.45, NNZs: 63, Bias: 11.148161, T: 36728, Avg. loss: 0.234619\nTotal training time: 0.07 seconds.\n\n\n\nTotal training time: 0.07 seconds.\nNorm: 27.30, NNZs: 63, Bias: -17.234162, T: 36728, Avg. loss: 0.023293-- Epoch 4\n\nTotal training time: 0.08 seconds.\nNorm: 43.09, NNZs: 63, Bias: -15.777237, T: 36728, Avg. loss: 0.226653Norm: 17.77, NNZs: 63, Bias: -28.526351, T: 36728, Avg. loss: 0.003036\nTotal training time: 0.07 seconds.\n\nTotal training time: 0.08 seconds.\nNorm: 47.58, NNZs: 63, Bias: -48.422860, T: 36728, Avg. loss: 0.083446-- Epoch 4\n-- Epoch 4\n\nTotal training time: 0.08 seconds.\nNorm: 37.87, NNZs: 63, Bias: -13.307105, T: 36728, Avg. loss: 0.145558\nNorm: 40.02, NNZs: 63, Bias: -0.634368, T: 36728, Avg. loss: 0.268030\nTotal training time: 0.08 seconds.\nTotal training time: 0.08 seconds.\n-- Epoch 5\nNorm: 42.08, NNZs: 63, Bias: -31.930273, T: 45910, Avg. loss: 0.079105\nTotal training time: 0.09 seconds.\n-- Epoch 5-- Epoch 5\n\n-- Epoch 5-- Epoch 5Norm: 33.01, NNZs: 63, Bias: 2.422221, T: 45910, Avg. loss: 0.289599Norm: 44.67, NNZs: 63, Bias: -16.959813, T: 45910, Avg. loss: 0.098675\n\nTotal training time: 0.09 seconds.\n-- Epoch 5\n\n\n-- Epoch 5\nTotal training time: 0.10 seconds.\n-- Epoch 5Norm: 24.67, NNZs: 63, Bias: -16.049084, T: 45910, Avg. loss: 0.017908Norm: 16.31, NNZs: 63, Bias: -27.582375, T: 45910, Avg. loss: 0.002493\n\nTotal training time: 0.10 seconds.\nNorm: 43.98, NNZs: 63, Bias: -44.692505, T: 45910, Avg. loss: 0.069981Norm: 34.59, NNZs: 63, Bias: 9.500824, T: 45910, Avg. loss: 0.199661-- Epoch 5\nTotal training time: 0.10 seconds.\n\n\nTotal training time: 0.10 seconds.\n\n-- Epoch 5Norm: 34.48, NNZs: 63, Bias: -12.599060, T: 45910, Avg. loss: 0.118288\n\nTotal training time: 0.10 seconds.\nTotal training time: 0.09 seconds.\nNorm: 40.02, NNZs: 63, Bias: -3.064517, T: 45910, Avg. loss: 0.268254\nTotal training time: 0.10 seconds.\nNorm: 39.45, NNZs: 63, Bias: -14.152008, T: 45910, Avg. loss: 0.188412\nTotal training time: 0.11 seconds.\n-- Epoch 5-- Epoch 6\n\nNorm: 39.32, NNZs: 63, Bias: -28.791110, T: 55092, Avg. loss: 0.065990\nTotal training time: 0.11 seconds.\nNorm: 37.02, NNZs: 63, Bias: -0.831084, T: 45910, Avg. loss: 0.218267\nTotal training time: 0.10 seconds.\n-- Epoch 6\nNorm: 41.54, NNZs: 63, Bias: -15.381686, T: 55092, Avg. loss: 0.079861\nTotal training time: 0.11 seconds.\n-- Epoch 6-- Epoch 6\n\n-- Epoch 6\nNorm: 41.00, NNZs: 63, Bias: -41.973028, T: 55092, Avg. loss: 0.055768Norm: 37.64, NNZs: 63, Bias: -3.836115, T: 55092, Avg. loss: 0.238960\nTotal training time: 0.11 seconds.\n-- Epoch 6-- Epoch 6-- Epoch 6\nTotal training time: 0.12 seconds.\nNorm: 30.70, NNZs: 63, Bias: 2.397262, T: 55092, Avg. loss: 0.251056\n\n\nTotal training time: 0.12 seconds.\n-- Epoch 6\n\nNorm: 22.48, NNZs: 63, Bias: -15.061767, T: 55092, Avg. loss: 0.014129Norm: 32.73, NNZs: 63, Bias: 8.121529, T: 55092, Avg. loss: 0.178073Norm: 36.96, NNZs: 63, Bias: -12.591127, T: 55092, Avg. loss: 0.163850Norm: 15.59, NNZs: 63, Bias: -26.796663, T: 55092, Avg. loss: 0.002312\nTotal training time: 0.12 seconds.\n\nTotal training time: 0.13 seconds.\n\nTotal training time: 0.12 seconds.\n\nTotal training time: 0.12 seconds.\n-- Epoch 6-- Epoch 6\n\nNorm: 32.42, NNZs: 63, Bias: -11.812112, T: 55092, Avg. loss: 0.102905Norm: 34.63, NNZs: 63, Bias: -0.644378, T: 55092, Avg. loss: 0.191334\nTotal training time: 0.13 seconds.\n\nTotal training time: 0.12 seconds.\nConvergence after 6 epochs took 0.13 seconds\nConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.14 secondsConvergence after 6 epochs took 0.14 secondsConvergence after 6 epochs took 0.14 seconds\n\n\nConvergence after 6 epochs took 0.14 secondsConvergence after 6 epochs took 0.14 seconds\nConvergence after 6 epochs took 0.15 seconds\nConvergence after 6 epochs took 0.14 seconds\n-- Epoch 7Convergence after 6 epochs took 0.14 seconds\n\n\nNorm: 32.96, NNZs: 63, Bias: -0.478620, T: 64274, Avg. loss: 0.172945\nTotal training time: 0.14 seconds.\nConvergence after 7 epochs took 0.15 seconds\nCPU times: user 1min 36s, sys: 2min 26s, total: 4min 3s\nWall time: 16.6 s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'alpha': 0.0001, 'max_iter': 30000}"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': [1E-4, 1E-2, 1E-1, 1, 1E1],\n",
    "    'max_iter': [1000, 10000, 30000]\n",
    "}\n",
    "\n",
    "grid_sgd_clf = GridSearchCV(sgd_clf, param_grid)\n",
    "%time grid_sgd_clf.fit(X_train_prepared, y_train)\n",
    "grid_sgd_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7931035344073936"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "grid_sgd_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 2, 7, ..., 4, 4, 4])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "y_predict = grid_sgd_clf.predict(X_test_prepared)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test Report\n               precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99       235\n           1       1.00      0.92      0.96       267\n           2       0.98      0.91      0.94       272\n           3       0.96      0.91      0.94       234\n           4       0.72      1.00      0.83       255\n           5       0.97      0.63      0.77       204\n           6       0.94      0.89      0.92       228\n           7       0.78      0.96      0.86       225\n           8       0.50      0.94      0.65       211\n           9       1.00      0.08      0.15       216\n          10       1.00      1.00      1.00       204\n\n    accuracy                           0.85      2551\n   macro avg       0.89      0.84      0.82      2551\nweighted avg       0.90      0.85      0.83      2551\n\n"
    }
   ],
   "source": [
    "report_sgd_clf = classification_report(y_test, y_predict)\n",
    "print(\"Test Report\\n\", report_sgd_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(n_jobs=-1)"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "random_forest_clf.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = random_forest_clf.predict(X_train_prepared)\n",
    "y_predict_test = random_forest_clf.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test Report for train\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       854\n           1       1.00      1.00      1.00      1033\n           2       1.00      1.00      1.00      1012\n           3       1.00      1.00      1.00      1033\n           4       1.00      1.00      1.00      1034\n           5       1.00      1.00      1.00       942\n           6       1.00      1.00      1.00       822\n           7       1.00      1.00      1.00       923\n           8       1.00      1.00      1.00       904\n           9       1.00      1.00      1.00       856\n          10       1.00      1.00      1.00       790\n\n    accuracy                           1.00     10203\n   macro avg       1.00      1.00      1.00     10203\nweighted avg       1.00      1.00      1.00     10203\n\nTest Report for test\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       235\n           1       1.00      0.99      0.99       267\n           2       1.00      1.00      1.00       272\n           3       0.99      1.00      0.99       234\n           4       1.00      0.99      0.99       255\n           5       1.00      1.00      1.00       204\n           6       0.99      1.00      0.99       228\n           7       1.00      1.00      1.00       225\n           8       1.00      1.00      1.00       211\n           9       1.00      1.00      1.00       216\n          10       1.00      1.00      1.00       204\n\n    accuracy                           1.00      2551\n   macro avg       1.00      1.00      1.00      2551\nweighted avg       1.00      1.00      1.00      2551\n\n"
    }
   ],
   "source": [
    "report_random_forest_clf_train = classification_report(y_train, y_predict_train)\n",
    "report_random_forest_clf_test = classification_report(y_test, y_predict_test)\n",
    "print(\"Test Report for train\\n\", report_random_forest_clf_train)\n",
    "print(\"Test Report for test\\n\", report_random_forest_clf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}